# -*- coding: utf-8 -*-
"""deeplearning2-2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T9sXImmPBv1sBNmBcS9HBI3BElGBLPl1
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf 
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import LSTM
import os
import pathlib
import json

# read the data and generate sequences
raw_data = np.load('/content/drive/MyDrive/Robot_Trials_DL.npy')
# print(raw_data)
# print(np.shape(raw_data))
# raw_data = np.stack((raw_data, raw_data), axis = 0)
# # print(np.shape(raw_data))
# for i in range(3):
#   raw_data = np.concatenate((raw_data, raw_data))
# print(np.shape(raw_data))
# noise_array = np.random.uniform(-100, 100, np.shape(raw_data))
# raw_data += noise_array
# print(raw_data)

# shuffle of the data
np.random.seed(0)
np.random.shuffle(raw_data)
# print(np.shape(raw_data))
# data = np.zeros(np.shape(raw_data))

# split the data
data_train = raw_data[0 : int(0.6 * np.shape(raw_data)[0])]
data_val = raw_data[int(0.6 * np.shape(raw_data)[0]) : int(0.8 * np.shape(raw_data)[0])]
data_test = raw_data[int(0.8 * np.shape(raw_data)[0]) : int(1.0 * np.shape(raw_data)[0])]

#convert to 2d array because minmaxscaler accepts only 2d arrays
convert1, convert2, convert3 = data_train.shape[0], data_train.shape[1], data_train.shape[2]
convert4, convert5, convert6 = data_val.shape[0], data_val.shape[1], data_val.shape[2]
convert7, convert8, convert9 = data_test.shape[0], data_test.shape[1], data_test.shape[2]

data_train = data_train.reshape(convert1*convert2, convert3)
data_val = data_val.reshape(convert4*convert5, convert6)
data_test = data_test.reshape(convert7*convert8, convert9)

scaler = MinMaxScaler()
scaler.fit(data_train[:, (0,2,3,4,5,6)])
data_train[:, (0,2,3,4,5,6)] = scaler.transform(data_train[:, (0,2,3,4,5,6)])
data_val[:, (0,2,3,4,5,6)] = scaler.transform(data_val[:, (0,2,3,4,5,6)])
data_test[:, (0,2,3,4,5,6)] = scaler.transform(data_test[:, (0,2,3,4,5,6)])

#turn it back into 3d now that minxmaxscaler was used
data_train = data_train.reshape(convert1, convert2, convert3)
data_val = data_val.reshape(convert4, convert5, convert6)
data_test = data_test.reshape(convert7, convert8, convert9)

data_train_x = data_train[:, :, (0, 2, 3, 4, 5, 6)]
data_train_y = data_train[:, :, 1:2]
data_val_x = data_val[:, :, (0, 2, 3, 4, 5, 6)]
data_val_y = data_val[:, :, 1:2]
data_test_x = data_test[:, :, (0, 2, 3, 4, 5, 6)]
data_test_y = data_test[:, :, 1:2]

# print('The size of the training data is : ' + str(np.shape(data_train)))
# print('The size of the validation data is : ' + str(np.shape(data_val)))
# print('The size of the testing data is : ' + str(np.shape(data_test)))

# build the network structure
initializer = tf.keras.initializers.Zeros()
model = tf.keras.Sequential()
model.add(tf.keras.layers.Masking(mask_value = 0)) # to tell sequence-processing layers that certain timesteps in an input are missing, and thus should be skipped when processing the data
# model.add(tf.keras.layers.GRU(32, activation = 'tanh', return_sequences = True, kernel_initializer = initializer, input_shape = (700, 6)))
model.add(tf.keras.layers.GRU(64, input_dim=64, input_shape = (700,6), return_sequences=True))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(1, activation = 'tanh'))

# initializer = tf.keras.initializers.Zeros()
# model = tf.keras.Sequential()
# model.add(tf.keras.layers.Masking(mask_value = 0)) # to tell sequence-processing layers that certain timesteps in an input are missing, and thus should be skipped when processing the data
# # model.add(tf.keras.layers.LSTM(64, activation = 'tanh', return_sequences = True, kernel_initializer = initializer, input_shape = (700, 6)))
# model.add(tf.keras.layers.LSTM(64, input_dim=64, input_shape = (700,6), return_sequences=True))
# model.add(tf.keras.layers.LSTM(32, return_sequences = True))
# model.add(tf.keras.layers.Dropout(0.5))
# model.add(tf.keras.layers.LSTM(16, return_sequences = True))
# model.add(tf.keras.layers.Dropout(0.5))
# model.add(tf.keras.layers.LSTM(8, return_sequences = True))
# model.add(tf.keras.layers.Dense(1, activation = 'tanh'))
#above is for LSTM model

# return sequences have to be true to propogate the mask layer correctly
patience = 15
# build another model
# model.summary()
# tf.keras.utils.plot_model(model)
# print(model.layers[0].weights)
model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'MeanSquaredError', metrics = 'mse')

callbacks = [EarlyStopping(monitor='val_mse', patience=15, verbose=1, mode='min'), 
            ModelCheckpoint(filepath='model.h5', verbose=1, monitor='val_mse', save_best_only=True, save_weights_only=False, mode='min')]
# train the model
history = model.fit(data_train_x, data_train_y, batch_size = 50, epochs = 400, validation_data = (data_val_x, data_val_y), shuffle = True, callbacks = callbacks)

result = model.predict(data_test_x)
plt.plot(result[0, :], 'r-', label = 'prediction')
plt.plot(data_test_y[0, :], 'b-', label = 'label')
plt.legend()
plt.show()